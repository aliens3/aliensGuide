---
layout: layouts/guide.njk
tags: stat
---

## 11.1

ここでは仮説検定について扱います．Section1で見たDr.Kの実験との関連についても述べますが，結論としてはDr.Kの実験のように効果の大きさに興味がある際には，仮説検定はあまり重要ではありません．仮説検定が最も有用なのは，差が存在するのかしないのか，ということだけに興味があるときです．

しかし，仮説検定で出てくる概念，例えばp値はそういう文脈以外の様々な論文でも登場するので，ここで説明します．また理論的には，ネイマンピアソンの補題など重要なものがあり，そちらも最後に紹介します．

## 11.1
問い：コインを10回投げて10回オモテが出た．このコインは公平なコインだろうか？{.pb-6}

この問いに仮説検定的に答えるのなら次のようになります．

*もしこれが公平なコインだと仮定する．つまり，オモテが出る確率$p = \frac{1}{2}$と考える．この場合，オモテが10回出る確率は$(\frac{1}{2})^{10}$であり，十分低い．よって，公平なコインとは考えられない．つまり，$p \neq \frac{1}{2}$である．*{.pb-6}

これは背理法です．つまり，最初にp=1/2を仮定して，最終的に，もしp=1/2とすれば，これが起きる確率が低すぎるのでp=1/2ではない．と結論づけています．途中の理屈はおいておいて，なぜ背理法を使うのか，もっと直接的にコインで公平でないことを示せないのか，という問題を考えてみます．

## 11.1

背理法を使う一番大きな理由は，pがある値と等しい，ということは示しがたいからです．例えば8回中8回オモテがでたのだから，オモテが出る確率は100%(p=1)なのではないかと予想したとしましょう．この予想については次のように反論できます．

*確かにp=1ならば8回中8回オモテがでる．しかし，p=0.999でも8回中8回オモテになるだろうし，p=0.9であっても8回中8回オモテになりうる．実際p=0.9のとき8回中8回オモテになるのは，0.9^8=0.43なので，2回に1回くらいはすべてオモテになる．だからpは0.90~1のどの値と等しくてもおかしくはないだろう．*

もっと一般には，コイン投げの結果から$p=p_1$<small>(ただし$p_1$は0から1までの定数)</small>と合理的に予想できる場合には，$p$が$p_1±Δ$のどこにあっても，その結果が生じうるので，その範囲にあれば，$p=p_1$でなくても矛盾はないということになります．

この反論をふまえると，逆にpが何であれば，その結果になりえないか，矛盾するのかを考えるのが筋が良いことが分かります．例えばp=0なら，8回投げて8回オモテは絶対にありえない．p=0.01でも全部オモテになる確率は，0.00000001%だからまずないだろう，など．これはすでに背理法の思考になっています．つまり，形式的に整理すれば

p=0.01と仮定する．このとき8回オモテの確率0.00000001%である．これは十分に小さいので，仮定は誤りでp=0.01ではないと考えられる．

ということになります．最初の回答はこれをp=1/2にしたバージョンです．このように，$p=p_1$という主張は肯定することは難しくても，否定することは容易なのです．

## 11.1

では，十分小さいというのはどれくらいの確率なのでしょうか．ひとまずは，恣意的に5%としましょう．つまり，p=1/2が正しいなら5%以下でしか起きないような出来事が起こったとき，p=1/2を否定することにする，ということです．この基準を採用すれば，例えば，コインを4回投げて4回表だった場合には，確率1/16=6.25%で起きることなので，p=1/2は否定できません．コインを5回投げて5回表だった場合には，確率1/32=3%で起きることなので，p=1/2は否定できます．

ここで注意すべき点があります．それは、コインを1000回投げて500回表になる確率は約2.5%になることを考えると分かります．先程の理屈から，これは，5%以下でしか起きないことなので，1000回投げてオモテが500回ならp=1/2を否定すべきなのでしょうか？しかし，もし本当にp=1/2ならオモテが半分である確率が最も高いはずで，最もよく生じることが起きたことを否定の根拠とするのは論理として破綻しています．このようなことが起きる理由は、コインを投げる回数を増やせば，ぴったり特定の回数となる確率はどんどん減っていくことにあります．

だからp=1/2を否定するのは，5%以下でしか起きないような**極端な**出来事が起こったとき，とすべきです．例えばコインを10回投げた場合でもう一度考えよう．コインを投げたときにオモテがk回出る確率の分布は次のようになる．




この分布から，p=1/2が正しければ，もっともオモテが出やすいのは5回で，この最も出やすい場合から離れていくと，オモテの回数が2~8回の間に入る確率は


で約90%だと分かります．逆にオモテが0-1,9-10回出る，つまり，最も出やすい5回から離れた極端な場合が起きる確率は　です．ということは，オモテが0,1,9,10回出るのが5%しかでない極端な場合であり，例えばオモテが8回出る確率はそれ単独では起きる確率は1/256で5%以下ですが極端な話とは見なされないことになります．

以上の話は5%を別の確率αに変えても全く同じです．例えば，α=0.3，つまり30%とすれば，


一方，α=0.01とすれば，

p=1/2が真実だと仮定ときに，確率αでしか起きない極端な事象が生じれば，p=1/2を否定する．という理屈なので，αを大きくすれば「極端」の基準がゆるくなって，p=1/2は否定されやすくなりますし，αを小さくすれば，よっぽどまれな事象が起きない限り極端とは見なさないので，p=1/2は否定されにくくなります．<small>()</small>

## 

それではαをいくつにするのが良いのか，ということが次に気になることです．恣意的にα=0.05と定めましたが，何か客観的な方法はないのでしょうか．結論から言うとありません．というのも，αをいくつにするか，という問題は，それがαをある値に定めることの帰結を理解した上で状況に応じて決断すべきことだからです．

コイン投げの例を，もう一度整理しながら見ていきます．

*もしこれが公平なコインだと仮定する．つまり，オモテが出る確率$p = \frac{1}{2}$と考える．この場合，オモテが10回出る確率は$(\frac{1}{2})^{10}$であり，十分低い．よって，公平なコインとは考えられない．つまり，$p \neq \frac{1}{2}$である．*{.pb-6}

最初にp=1/2を仮定していますが，この仮定を**帰無仮説**と呼び，$H_0$で表すことにします．一方，この帰無仮説の否定，p≠1/2を**対立仮説**とよび，$H_1$で表すことにします．よって，帰無仮説と対立仮説は合わせると，pの値について漏れ・ダブリなく全ての場合を考えています．だから，帰無仮説と対立仮説のどちらかは正しいわけです．

このとき，真実と結論の関係は次のように2*2の表で整理できます．


番号を振ったのでそれぞれ説明しましょう．

1. 真実がH0でないとき，正しくH0を否定(一般的には**棄却**と言う)して，H1を結論づける場合．
2. 真実がH0なのに，たまたま棄却域の事象が生じて，H0を棄却し，H1を誤って結論づける場合．

まず，今回のコイン投げの例での結論はH1ですが，1,2の場合のどちらが起こっているのかを知る方法は(少なくとも今回の結果だけを使うのであれば)存在しません．というのも，もし本当のpの値が0.95なら，最初の回答はこれを見抜いて，p=1/2を否定できたことになり，1に当てはまることになりますし，本当のpの値が0.5だったのなら，最初の回答は，たまたま極端な場合続が出たせいで，H1を採用してしまっていることになり，2に当てはまるからです．しかし，本当のpの値は知りようがないので，1,2どちらなのかを知ることはできません．

しかし，これらが生じる確率を制御することはできます．なぜなら2が生じる確率はαだからです．コイン投げの例に即して考えると「p=1/2が真実だと仮定ときに，確率αでしか起きない極端な事象が生じれば，p=1/2を否定する」ということでしたが，これは「H0が真実のときに，確率αで起きる極端な事象が生じれば，H0を棄却する」ということなので，まさに2.が生じる確率をαが表していることが分かります．

次に3.4を見ていきます．

3. 真実がH0のとき，棄却域の事象が生じないで，正しくH0を棄却しない場合．
4. 真実がH1なのに，棄却域の事象が生じないために，誤ってH0を棄却しない場合．

まず，3,4の結論と1,2の結論を見比べてください．1,2では結論はH1ですが，3,4では結論はH0を棄却(否定)しないということになります．つまり，3,4はH0を結論づけるわけではないのです．これは最初に$p=p_1$を示すことはできないという説明と同じ理屈によります．

例えば，コイン投げの例でオモテが10回ではなく5回でたと考えてみましょう．直観的にも確率的にも公平なコインであることは否定はできません．しかし，だからといって，p=1/2を結論づけることもできません．というのも，p=0.4とか，p=0.6とかであれば，5回オモテになることは十分ありえるからです．もっと一般にはpは1/2周辺ならどの値もあり得るわけで，p=1/2だけが正しいと結論づけることはできないのです．それゆえ，この結果からはp=1/2でないとは言えませんし，p=1/2であるとも言えません．つまり，棄却域の事象が生じないときには，H0を肯定することも否定することもしない，という態度をとることになります．

1,2の場合と同様に真実がH0,H1のどちらなのかを私たちが知ることはないので，H0を棄却しない場合，3,4のどちらなのかを知ることはできません．

従って，1,2と同様にこれらが生じる確率を制御することを目指します．例えば，3が生じる確率は1-αです．なぜならαとは，コイン投げの例に即して考えると「p=1/2が真実だと仮定ときに，確率αでしか起きない極端な事象が生じれば，p=1/2を否定する」ということで，これを言い換えると，「H0が真実のときに，1-αで極端な場合が生じず，H0を否定しない」ということになり，これはまさしく3.が生じる確率を表しているからです．

ということで，あとは1と4の確率を制御することを目指します．αは2の誤りをする確率だったので，βを4の誤りをする確率としてみましょう．つまり，コイン投げの例に即して考えると「p ≠ 1/2が真実のときに，確率βで極端な場合が起こらず，p=1/2を棄却しない」ということになります．そうすると，これを言い換えて，「p ≠ 1/2が真実のときに，確率1-βで極端な場合が起こり，p=1/2を棄却する」となるので，2の確率は1-βであることが分かります．

以上から，表に確率を書いたものを載せましょう．


2の誤りは**第一種の誤り**とか**あわてんぼうの誤り**と呼ばれます．4の誤りは**第二種の誤り**とか**ぼんやりものの誤り**と呼ばれます．α=**あ**わてんぼう，β=**ぼ**んやりもの，のように，それぞれの誤りを犯す確率と頭文字があっているので，分かりづらい一種とか二種に加えて，このような名前がつけられています．そして1は正しく，かつ，結論が得られる唯一の場合(3は正しい場合ですが何の結論も得られません)なので，私たちがもっとも目指すべきもので，これが生じる確率は**1-β**で検出力と呼ばれます．


コイン投げの例では，第一種の誤りの可能性はαだと定量的に評価できています．では第二種の誤りを犯す可能性はどれだけあるのでしょうか．これを厳密に評価することは難しいので，直観的に図で理解してみよう．


ということはβとαと試行回数n，そして本当のpの値と帰無仮説p=1/2とのずれの関係は次の通りになります．

1. αを小さくするほどβは大きくなる
2. nを大きくするほどβは小さくなる
3. 帰無仮説からの真の値のズレが大きいほどβが小さくなり，逆に小さいほどβは大きくなる

この3つの事実からは仮説検定について色々な事実を述べることができますが，ひとまずは次のことを述べておきます．

- (1,2より)αとβはトレード・オフの関係にあるため，決まったnに対してαとβをどこまでも高くする，ということはできない．
- (1,2より)nが大きくできるなら，1の事実からαを小さくすることで大きくなったβを，2の事実からnを大きくすることで相殺して，αとβを両方小さくできる． 
- (2,3より)帰無仮説と真の値のズレが小さい時，βが大きいので，検出力1-βは小さい．つまり差があっても(H0でなくても)正しく見抜いて棄却できる確率が低いが，nを大きくすれば1-βを大きくすることができる，つまりnが大きければどんな小さい差でも見抜くことができる．


## 薬の効果に当てはめてみる

薬の効果を仮説検定的に調べてみます．前回までは$μ^D-μ^P$を区間推定しましたが，仮説検定的に$μ^D-μ^P$を調べてみると次のようになります．

帰無仮説H0 : $μ^D=μ^P$
対立仮説H1 : $μ^D > μ^P$

このように設定する理由を理解します．まず仮説検定では$μ^D-μ^P$が特定の値になることを示すことは困難です．これはコイン投げの例で$p=p_1$を直接示せなかったのと同じ理由です．そこで，帰無仮説として，望ましくない結果を仮定して，それを否定することを目標とすることにします．介入で望ましくないのは，無介入の場合と差がない場合で，これをH0とします．帰無(null)とは差がないという意味なので，これは帰無仮説と呼ばれます．

対立仮説が$μ^D ≠ μ^P$ではなく$μ^D > μ^P$なのは，$μ^D$の方が大きいことを示せないと，薬の効果があったとは言えないからです．

これを検証するためには，コイン投げと同じように考えます．コイン投げで極端な場合とは，帰無仮説が真実のときに生じる可能性が，5%以下であるような平均から外れた両端でした．薬の効果の場合にもα=0.05で行うとすれば，極端な場合とは，帰無仮説が真実のときに，生じる可能性が5%以下であるほど$μ^D - μ^P$が大きくなる場合だと考えられます．

実際にやってみます．まず，薬の効果については，

$$
t=\frac  { (\overline{X} - \overline{Y})  - (\mu^D -\mu^P) } {\sqrt{U_e(\frac{1}{m}+\frac{1}{n})}}
$$


が成り立ちました．詳しくは前回のここを見てください．帰無仮説より，$\mu^D -\mu^P=0$なので，
$$
t=\frac  { (\overline{X} - \overline{Y})  } {\sqrt{U_e(\frac{1}{m}+\frac{1}{n})}}
$$

と変形できます．この値tはサンプルから計算可能で，前回の結果

$\overline{X}=m^D,\overline{Y} = m^P,U_X=S_D^2,U_Y = S_P^2,m=50,n=50$

を代入して計算すると，

t=

となります．α=0.05の場合の棄却域はどこになるのでしょうか．tが自由度1998のt分布に従うとき，このサイトで調べると，

$P(t \leqq)=0.95$

であることが分かります．つまり，帰無仮説が正しいときに，tは95%の確率で　となるのです．(もちろんこの確率は7.12でやったとおりに解釈します)ということは，$ \leqq t$となるのは5%以下の極端な場合で，これが棄却域になります．

ということは，t= は棄却域に入っており，H0 を否定してH1を採用することになります．もしくは，P値の考え方を使うなら，tが自由度1998のt分布に従うとき，このサイトで調べると，

$P(\leqq t)=0.95$

なので，P値は となり，今回設定したα=0.05以下なので，棄却され，H1$μ^D > μ^P$が採用されます．つまり，薬群とプラセボ群には有意差があることが分かりました．

## なぜ使うべきではないか？

冒頭で薬の効果を調べる際には仮説検定は使わないと述べましたが，それは仮説検定で得られる結論H1$μ^D > μ^P$は私たちが知りたいものを十分に表してはいないからです．

$μ^D > μ^P$という事実は，薬を使うことを決断するのに決定的な事実ではありません．例えば，$μ^D=μ^P+0.01$でも$μ^D > μ^P$は成り立ちますが，これは薬を飲んだほうが0.1mmだけ身長が伸びるというものです．これしか効果がないのなら薬を使うことはないでしょう．つまり，薬の使用は単に$μ^D > μ^P$であることだけではなく，$μ^D$が$μ^P$よりも意味があるほど大きくなければいけないのです．仮説検定では，そのことを積極的に汲み取った主張をすることができません．

また，歴史的にはP値の誤用(悪用)が問題になりました．

まず，P値は，帰無仮説H0が正しいと仮定した場合の，起こった事象の極端さを表す値でした．つまり，P値が小さいほど，帰無仮説と起こった事象は両立しない，矛盾すると考えられるのです．この事実を拡大解釈して，P値が小さいほど，実験結果に重大性があるとか，効果に差があると考えてしまう人がいました．

しかし，P値はあくまで，帰無仮説のもとでの起こりづらさを表すに過ぎないものです．例えばコインでオモテが出る確率が0.501のコインがあります．つまり，0.1%だけ公平なコインよりもオモテが出やすいのです．このコインを1000億回投げたときに最も出やすいのは501億回オモテがでることがですが，この事象について帰無仮説p=0.5のもとでP値を計算すると，詳細は省略しますが，P= となります．なぜなら，こんなにたくさんコインを投げたときには，p=0.5であれば，500億回に近い値オモテが出るのが殆どで，それよりも1億回も多くオモテがでることはまずないからです．一方，最初に見たように，10回コインを投げて10回オモテの場合のP値は です．このようにP値はサンプルサイズなどに影響を受けるため，それ自体で何かを解釈することはできません．

ほとんど同じ話になりますが，今見たように，どんなに小さい差でもサンプルサイズを大きくすれば有意差がでます．これは先程

- (2,3より)帰無仮説と真の値のズレが小さい時，βが大きいので，検出力1-βは小さい．つまり差があっても(H0でなくても)正しく見抜いて棄却できる確率が低いが，nを大きくすれば1-βを大きくすることができる，つまりnが大きければどんな小さい差でも見抜くことができる．

と述べたことの意味です．だから，有意差があることは，その差に現実的な意義があることを意味しません．

また，複数の仮説を同時に検定してしまうことも問題です．例えば，2つの帰無仮説H_0とH_1があるとします．これらの帰無仮説が両方正しいとしましょう．そのときに，正しく判断=H0を棄却しない確率は，それぞれの仮説をα=0.05で検定する場合，(1-α)^2となります．一般に，n個の帰無仮説があって，それらが全て正しい場合，正しく判断できる確率は(1-α)^nとなり，nが大きいほど．帰無仮説のどれか誤って棄却してしまう確率は高くなります．

そんなにたくさんの仮説を同時に検定しないと思うかも知れませんが，次の状況では無意識のうちにこれを行ってしまっています．


だからこそ，実験を行う前に，帰無仮説と対立仮説を事前に設定して，何について調べるのかを決めておく，そして公式の研究であれば事前にそれを公表する必要があるのです．

統計学の始祖であるフィッシャーも，

*実験計画法と推測統計学の中心理論の根幹は，実験や観察を始める「前」に，実験区の割り付けを完了し，帰無仮説と対立仮説を設定し，仮説検定のための有意水準を決めることにある。実験終了後に統計学者に相談を持ちかけるのは，統計学者に，単に死後診察を行って下さいと頼むようなものである。統計学者はおそらく何が原因で実験が失敗したかという実験の死因について意見を述べてくれるだけであろう*

と述べています．

これを守らなければ，結果をとにかく集めて，

- 有意差がでない結果は捨てる．
- たまたま有意差が生じたものは論文にして発表する．

とすることで，発表された論文の多くがたまたま有意差が生じたものの集まりということになります．本来はこのような誤りはα(例えば医学研究ではα=5%)しか生じないはずなのに，どの結果を発表するのかにバイアスがかかっているため，αよりもずっと高い割合でたまたま生じた結果が含まれることになります．そうすると，結果が正しいのかを確かめるために，同じ実験をもう一度やってみると，たまたま稀に生じただけの結果なので，報告された効果が再現できないということになります．これは再現性の危機などと呼ばれて問題になりました．

## 区間推定は

このような問題点から，薬の効果のように，差の大きさに興味がある場合には，帰無仮説ではなく区間推定を行うべきです．一方，差が存在すること，つまり帰無ではないこと自体に興味がある場合には，仮説検定のほうが分かりやすいでしょう．例えばヒストン粒子の存在を示す実験では仮説検定が使われ，ヒストン粒子が存在する確率は99.99995%などと発表されました．これはα=0.00005で仮説検定したとイメージして大きく違いはないでしょう．つまりヒストン粒子が存在しないと仮定すると，観察された実験結果が起きる確率は，　であり，これは，αよりも低いので，棄却してヒストン粒子が存在することを主張しているわけです．

## 

それでは区間推定と仮説検定は別のものなのでしょうか．Dr.Kの実験に仮説検定を当てはめた場合を見て気づくように，これら2つは本質的に同じことをしています．

例えば，





つまり，2つの平均の差を危険率αで検定することと，2つの平均の差の信頼度（1−α）の信頼区間を推定することは同等なのです．しかし，検定では，H0を棄却するかしないかを主眼として他の情報を切り捨てるのに対して，区間推定では多くの情報を残すので，効果の大きさを読み取ることができます．差の有無そのものだけに興味があるときには，いらない情報を捨てたほうが分かりやすいので仮説検定がいいですが，効果に少しでも興味があるのなら，区間推定の形にするのがよいでしょう．

## おまけ

それでも，仮説検定で薬の効果を知りたい場合にはどうすればいいのでしょうか．問題はサンプルサイズをむやみに増やすと，どんなに小さい差でも検出されてしまうことです．それゆえ

1. 差がどれくらいあるかを予想する．
2. αを設定する．
3. 差とαから検出力を確保するために必要なサンプルサイズを計算する

ということになります．例えば，Dr.Kの実験では差が1だと予想して，αは0.05に設定しました．検出力については露わに考えてはいませんでしたが，実は誤差を0.5に設定することで，効果が1の場合にぎりぎり，95%で有意差が生じるようなサンプルサイズになっています．



https://www.jstage.jst.go.jp/article/grass/66/4/66_209/_pdf/-char/ja
https://www.jstage.jst.go.jp/article/sjpr/59/1/59_123/_pdf


## 一般的な意思決定について

\\(\theta \in\Theta_0\\)のときは行動Aを行う
\\(\theta \in\Theta_0^c\\)のときは行動Aを行わない

母パラメータ\\(\theta\\)で規定される確率分布から生成されるサンプルデータ\\(X_k\\)から計算されるXについての棄却域をRとして，
\\(\beta(\theta) = P_{\theta}(X \in R ) \\)とすると，
\\(\beta(\theta)　\theta \in\Theta_0\\)が第一種の誤りで，
\\(1 - \beta(\theta)　\theta \in\Theta_0^c\\)が第二種の誤り．

\\( \beta(\theta) =\begin{cases} 0 & (\theta \in\Theta_0) \\\\ 1 & (\theta \in\Theta_0^c) \end{cases}\\)

となることが理想的．だが，トレードオフなので，αの上限を定めたままβを最大化できれば，それが'良い'検定．
そのときに，数学的に色々な定理があって(ネイマンピアソンの補題，カーリン・ルビンの定理)，今回の場合には，今回採用した方法が最も良い検定であることが証明されている．

詳しくは　https://starpentagon.net/analytics/8_3_1_error_prob_power_function/ 


















================================================================================================================================================================================================================================================================================================================
